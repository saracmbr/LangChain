{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c03b063-e6ec-4375-bca1-fad5ecfe1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07004fcf-cf05-4b31-bf3a-7cbe1fdeda70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain_nvidia_ai_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "30f3edc7-4b97-44d1-a780-a45403b41ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader,TextLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "from torch import cuda\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#from langchain_community.llms import Ollama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "#from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_core.runnables import Runnable\n",
    "import torch\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ae96531-aa16-4328-9d4c-7bf56c618f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to empty gpu ram\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "64ea814d-81bc-4b88-81b4-c76b26ffc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"qwen2:7b-instruct\",num_gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca69c28e-cfb3-4871-a2f5-31afe5de8b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't ice creams ever play hide and seek?\\n\\nBecause they always get frozen out!\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ba0fc98-7e0b-4fa5-8d4c-c530c0ae4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16\",num_gpu=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "83f184e1-4897-4855-a513-627a1ad39aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "45704bcc-b019-423b-8523-1b8924fdf0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "NVIDIA_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9da540fc-9580-4d81-ac4e-479c16d02769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='baai/bge-m3', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-e5-v5', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embed-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-nv-embed-v1'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='snowflake/arctic-embed-l', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-arctic-embed-l'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='NV-Embed-QA', model_type='embedding', client='NVIDIAEmbeddings', endpoint='https://ai.api.nvidia.com/v1/retrieval/nvidia/embeddings', aliases=['ai-embed-qa-4', 'playground_nvolveqa_40k', 'nvolveqa_40k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-mistral-7b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "NVIDIAEmbeddings.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "89c6b204-4a6f-48c9-a8f8-16fdf0518c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-mistral-7b-v2\", \n",
    "  nvidia_api_key=NVIDIA_API_KEY, \n",
    "  truncate=\"NONE\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4d4521e8-c017-4c04-9c0f-ce08749e6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"/home/sara/Documents/Advanced_Rag/Udemy-Advanced-LangChain-main/data/food.txt\",encoding = 'UTF-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eb022371-2a55-4eb9-aab3-e93dec89d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the file and put them into a variable called raw_text\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \"\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1300,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=100,  # the number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS\n",
    "    \n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "\n",
    "for doc in documents:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a9a4bbd9-e861-4723-a41d-fad84bfcec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(\n",
    "    docs_processed, embedder, distance_strategy=DistanceStrategy.JACCARD\n",
    ")\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "80e322d3-46a8-436f-bff1-4f9aed136046",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a helpful assistant for our restaurant. you are going to answer questions about the food on the menu based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer here:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "32fac9f0-6c78-470b-b563-1cdbd7f2f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the most expensive food on the menue ', 'result': 'The most expensive food on the menu is \"caponata\" priced at $21.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n",
    "\n",
    "result = qa.invoke(input=\"What is the most expensive food on the menue?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "933e43ad-ca00-4fd3-9231-68fd90197fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b37368bc-4564-4e09-a8f9-a2ae61f72922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most expensive food on the menu is \"branzino\" which costs $21. This is described as Mediterranean sea bass, usually grilled or baked.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What is the most expensive food on the menue ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cea1ee62-668b-4dfd-86f1-97bc71b040ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "REPHRASE_TEMPLATE = PromptTemplate.from_template(rephrase_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9a026eb4-9df1-49ba-b93a-cd79951a93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant for our restaurant. you are going to answer questions about the food on the menu based on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "11e35597-2f08-4bae-ac92-b1ce1099aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "rephrase_chain = REPHRASE_TEMPLATE | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31cff6da-0894-462e-90e0-05586238d34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Branzino the most expensive dish on the menu, and does it cost $21?'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrase_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"No, Are you sure this is the most expensive dish on the menu?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What is the most expensive food on the menue?\"),\n",
    "            AIMessage(content=\"The most expensive food on the menu is Branzino, which costs $21.\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "373d9487-573b-4cb8-99ed-c132bafda41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | ANSWER_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "544a9612-84ae-4986-9383-2c54a6b59fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = rephrase_chain | retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b5a42d78-2abc-418d-bfa5-aa8956e933fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, Branzino is one of the more expensive dishes on the menu based on the provided context. It costs $21, which positions it as a higher-priced main dish option in the list of restaurant items you've shared.\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"No, really?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What is the most expensive food on the menue?\"),\n",
    "            AIMessage(content=\"The most expensive food on the menu is Branzino, which costs $21.\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
